<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Adam Soliev – Portfolio</title>
    <!-- Typeface -->
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../style.css" />
    <link rel="icon" type="image/svg+xml" href="../assets/favicon.svg">
  </head>
  <script>
    function setActiveFromHash() {
      const hash = location.hash || "#projects";
      document.querySelectorAll(".nav-link").forEach(l =>
        l.classList.toggle("active", l.getAttribute("href") === hash)
      );
    }
    window.addEventListener("hashchange", setActiveFromHash);
    setActiveFromHash(); // run on initial load
  </script>
  <body class="ibm-plex-mono-regular">
    <!-- ─────────────────────  SINGLE SHEET  ───────────────────── -->
    <div class="sheet">
        <!-- ────────────────────  TOP BAR  ──────────────────── -->
        <header class="top-bar">
          <div class="name-colors">
            <h1 class="site-name">Adam Soliev</h1>
            <div class="color-bar">
              <!-- Color rectangles to mimic the reference image -->
              <div class="color-rect red"></div>
              <div class="color-rect green"></div>
              <div class="color-rect blue"></div>
              <div class="color-rect cyan"></div>
              <div class="color-rect yellow"></div>
              <div class="color-rect magenta"></div>
              <div class="color-rect black"></div>
              <div class="color-rect light-gray"></div>
              <div class="color-rect gray"></div>
              <div class="color-rect darker-gray"></div>
              <div class="color-rect darkest-gray"></div>
              <div class="color-rect dark-gray"></div>
              <div class="color-rect black-2"></div>
              <div class="color-rect light-green"></div>
              <div class="color-rect dark-green"></div>
              <div class="color-rect navy"></div>
              <div class="color-rect royal-blue"></div>
              <div class="color-rect maroon"></div>
              <div class="color-rect brown"></div>
              <div class="color-rect orange"></div>
            </div>
          </div>
          <nav class="main-nav" aria-label="Primary">
            <a href="../index.html" class="nav-link">Projects</a>
            <a href="../index.html#blog" class="nav-link active">Blog</a>
            <a href="../index.html#about" class="nav-link">About</a>
          </nav>
        </header>

        <!-- ─────────────────────  POST  ───────────────────── -->
        <section class="post">
            <h1>Retrieval Models</h1>
            <p><i>1 May 2025</i></p>

            <p>What is a retrieval model? <i>A retrieval model</i> is a mathematical framework that describes how information is retrieved
                from a collection of documents, providing a way to measure the [topical, user-specific and binary or multi-valued] relevance of a document to a given query. <i>A ranking algorithm</i>
                then ranks the documents based on their relevance. </p>
            <h2>Symbolic/Exact-match</h2>
            <ul>
                <li>Boolean retrieval (1960s) - assumes that each retrieved document needs to contain all terms of the query. Otherwise,
                    the document is non-relevant. It doesn't differentiate between retrieved documents (they are all assumed to be relevant) and
                    has no notion of ranking. It also puts search-efficiency burden on the user since simple queries don't work well.</li>
            </ul>

            <h2>Algebraic weighting</h2>
            <ul>
                <li>
                    <p>
                        Vector space (1960s-1970s)
                        - simply put, this model assumes that documents and queries are points in a high-dimensional weight space.
                        In this context, a distance between document-and-query points can be taken as a measure of relevance. More commonly though some sort of
                        similarity measure is used. The most successful such measure is the <i>cosine correlation</i>,
                        which measures the cosine of the angle between the document and query vectors and is defined as:
                    </p>

                    <!-- Cosine(Dᵢ, Q)  -------------------------------------------------------->
                    <math display="block">
                        <mrow>
                        <!-- Cosine(D_i, Q) -->
                        <mi>Cosine</mi><mo>(</mo><msub><mi>D</mi><mi>i</mi></msub><mo>,</mo><mi>Q</mi><mo>)</mo>
                        <mo>=</mo>

                        <!--  ─────────────────────────────────────────────────────────────────── -->
                        <mfrac>

                            <!-- Numerator:  Σ_{j=1}^{t} d_{ij}·q_j -->
                            <mrow>
                            <munderover> <mo>&#x2211;</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>t</mi>
                            </munderover>
                            <msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub>
                            <mo>&#x2062;</mo>  <!-- invisible multiplication -->
                            <msub><mi>q</mi><mi>j</mi></msub>
                            </mrow>

                            <!-- Denominator:  √( Σ d_{ij}² · Σ q_j² ) -->
                            <mrow>
                            <msqrt>
                                <mrow>
                                <munderover> <mo>&#x2211;</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>t</mi>
                                </munderover>
                                <msubsup><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup>
                                <mo>&#x2062;</mo>  <!-- invisible multiplication -->
                                <munderover> <mo>&#x2211;</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>t</mi>
                                </munderover>
                                <msubsup><mi>q</mi><mi>j</mi><mn>2</mn></msubsup>
                                </mrow>
                            </msqrt>
                            </mrow>

                        </mfrac>
                        <!--  ─────────────────────────────────────────────────────────────────── -->
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><msub><mi>D</mi><mi>i</mi></msub></math></td>
                            <td>vector of term-weights for document <var>i</var></td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><mi>Q</mi></math></td>
                            <td>vector of term-weights for the query</td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></math></td>
                            <td>weight of term&nbsp;<var>j</var> in document&nbsp;<var>i</var></td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><msub><mi>q</mi><mi>j</mi></msub></math></td>
                            <td>weight of term&nbsp;<var>j</var> in the query</td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><mi>t</mi></math></td>
                            <td>number of distinct terms (vector dimensionality)</td>
                        </tr>
                        <tr>
                            <td class="symbol">numerator</td>
                            <td>dot product of D and Q vectors</td>
                        </tr>
                        <tr>
                            <td class="symbol">denominator</td>
                            <td>product of Euclidean lengths of D and Q vectors</td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        Note from the formula that this model does take into account term weights and the number of matching terms, which
                        isn't possible in Boolean retrieval.
                    </p>
                    <p>
                        Most commonly used term weights, for both document and query vectors, are based on variations of <i>tf-idf</i>. The <i>tf</i>
                        reflects the importance of a term in a document or query, while the <i>idf</i> reflects the importance of a term in the collection
                        of documents (if it occurs in more documents, it is less differentiating).
                    </p>
            </ul>

            <h2>Probabilistic Models</h2>
            <ul>
                <li>
                    <p>
                        Binary-Independence (1970s-1980s) - treats a document as a bag of words, each encoded as either 1 (the query term is in the document) or 0, and
                        makes a strong assumption that terms are independent of each other. It defines document relevance as
                        <math>
                            <mi>P</mi><mo>(</mo><mi>R</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                            <mo>></mo>
                            <mi>P</mi><mo>(</mo><mi>NR</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                        </math>.
                        Applying Bayes' rule to each side and removing cancelled <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>)</mo></math>, you get:
                    </p>
                    <math display="block">
                        <mrow>
                            <mfrac>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>R</mi><mo>)</mo>
                                </mrow>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>NR</mi><mo>)</mo>
                                </mrow>
                            </mfrac>
                            <mo>></mo>
                            <mfrac>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>NR</mi><mo>)</mo>
                                </mrow>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>R</mi><mo>)</mo>
                                </mrow>
                            </mfrac>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol">leftside</td>
                            <td>likelihood ratio</td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        The likelihood ratio tells us how likely a document belongs to the relevant set and hence can be used to rank documents.
                        Keeping the aforementioned assumptions (binary and independence) in mind, you can estimate
                        <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>R</mi><mo>)</mo> </math>  by the product
                        of the individual term probabilities
                        <math>
                        <mrow>
                            <munderover>
                            <mo>&prod;</mo>
                            <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                            <mi>t</mi>
                            </munderover>
                            <mi>P</mi><mo>(</mo><msub><mi>d</mi><mi>i</mi></msub><mo>|</mo><mi>R</mi><mo>)</mo>
                        </mrow>
                        </math>
                        (and similarly for <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>NR</mi><mo>)</mo> </math>).
                    </p>
                    <p>
                        Another way to express the score is:
                    </p>

                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <mrow style="margin-right:10px;">
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <msub><mi>p</mi><mi>i</mi></msub>
                            <msub><mi>s</mi><mi>i</mi></msub>
                            </mfrac>
                        </mrow>
                        <!-- <mo>&#x2062;</mo> -->
                        <mo>&#x22C5;</mo>
                        <mrow>
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <mrow>
                                <mo>(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>p</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            <mrow>
                                <mo>(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            </mfrac>
                        </mrow>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><msub><mi>p</mi><mi>i</mi></msub></math></td>
                            <td>prob. term <var>i</var> occurs in a document from the rel. set</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>s</mi><mi>i</mi></msub></math></td>
                            <td>prob. term <var>i</var> occurs in a document from the non-rel. set</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>d</mi><mi>i</mi></msub></math></td>
                            <td>binary indicator (1 or 0)</td>
                        </tr>
                        <tr>
                            <td class="symbol">first product</td>
                            <td>contribution from terms present in the document</td>
                        </tr>
                        <tr>
                            <td class="symbol">second product</td>
                            <td>contribution from terms absent from the document</td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        With a bit of math manipulation, you can rewrite it as:
                    </p>
                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <!-- product over terms present -->
                        <mrow style="margin-right:10px;">
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <mrow>
                                <msub><mi>p</mi><mi>i</mi></msub>
                                <mo stretchy="false">(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo>
                            </mrow>
                            <mrow>
                                <msub><mi>s</mi><mi>i</mi></msub>
                                <mo stretchy="false">(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo>
                            </mrow>
                            </mfrac>
                        </mrow>
                        <mo>&#x22C5;</mo>
                        <!-- product over terms absent -->
                        <mrow>
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <mrow>
                                <mn>1</mn><mo>&#x2212;</mo><msub><mi>p</mi><mi>i</mi></msub>
                            </mrow>
                            <mrow>
                                <mn>1</mn><mo>&#x2212;</mo><msub><mi>s</mi><mi>i</mi></msub>
                            </mrow>
                            </mfrac>
                        </mrow>
                        </mrow>
                    </math>

                    <p>
                        The product on the right is identical for every document, so you drop it. To avoid accuracy problems caused by
                        working with small numbers, you use the logarithm (which converts the product to a sum):
                    </p>
                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <munder>
                            <mo>∑</mo>
                            <mrow>
                            <mi>i</mi><mo>:</mo>
                            <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn>
                            </mrow>
                        </munder>
                        <mi>log</mi>
                        <mrow>
                            <mfrac>
                            <mrow>
                                <msub><mi>p</mi><mi>i</mi></msub>
                                <mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            <mrow>
                                <msub><mi>s</mi><mi>i</mi></msub>
                                <mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            </mfrac>
                        </mrow>
                        </mrow>
                    </math>

                    <p>If nothing is known about relevant documents, set
                        <math>
                        <msub><mi>p</mi><mi>i</mi></msub>
                        <mo>=</mo>
                        <mn>0.5</mn>
                        </math>
                        (maximum uncertainty) and approximate
                        <math>
                        <msub><mi>s</mi><mi>i</mi></msub>
                        <mo>≈</mo>
                        <mfrac>
                            <msub><mi>n</mi><mi>i</mi></msub>
                            <mi>N</mi>
                        </mfrac>
                        </math>
                        (collection frequency). Plugging those into the weight gives:
                    </p>

                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <mi>log</mi>
                        <mrow>
                            <mo>(</mo>
                            <mfrac>
                            <mrow>
                                <mn>0.5</mn>
                                <mo stretchy="false">(</mo>
                                <mn>1</mn>
                                <mo>&#x2212;</mo>
                                <mfrac>
                                <msub><mi>n</mi><mi>i</mi></msub>
                                <mi>N</mi>
                                </mfrac>
                                <mo stretchy="false">)</mo>
                            </mrow>
                            <mrow>
                                <mfrac>
                                <msub><mi>n</mi><mi>i</mi></msub>
                                <mi>N</mi>
                                </mfrac>
                                <mo stretchy="false">(</mo>
                                <mn>1</mn>
                                <mo>&#x2212;</mo>
                                <mn>0.5</mn>
                                <mo stretchy="false">)</mo>
                            </mrow>
                            </mfrac>
                            <mo>)</mo>
                            <mo>=</mo>
                            <mi>log</mi>
                            <mrow>
                            <mo>(</mo>
                            <mfrac>
                                <mrow>
                                <mi>N</mi>
                                <mo>&#x2212;</mo>
                                <msub><mi>n</mi><mi>i</mi></msub>
                                </mrow>
                                <msub><mi>n</mi><mi>i</mi></msub>
                            </mfrac>
                            <mo>)</mo>
                            </mrow>
                        </mrow>
                        </mrow>
                    </math>

                    <p>
                        which is exactly the inverse‑document‑frequency (IDF) weight. Thus BIM naturally collapses to “sum of IDFs” when
                        no relevance feedback is available.
                    </p>

                </li>
                <li>
                    <p>
                        BM25 family (1990s) - a logical extension of BIM where term frequency and document length are taken into account, in addition to IDF.
                    </p>
                    <math display="block">
                        <mrow>
                        <mi>BM25</mi><mo>(</mo><mi>d</mi><mo>,</mo><mi>q</mi><mo>)</mo>
                        <mo>=</mo>
                        <munder>
                            <mo>&#x2211;</mo>
                            <mrow>
                            <msub><mi>t</mi><mi>i</mi></msub>
                            <mo>&#x2208;</mo>
                            <mi>q</mi>
                            </mrow>
                        </munder>
                        <mrow>
                            <mi>log</mi>
                            <mo>(</mo>
                            <!-- IDF term -->
                            <mfrac>
                            <mrow>
                                <mi>N</mi><mo>&#x2212;</mo><msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mn>0.5</mn>
                            </mrow>
                            <mrow>
                                <msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mn>0.5</mn>
                            </mrow>
                            </mfrac>
                            <mo>&#x22C5;</mo>
                            <!-- document‐side TF/length normalization -->
                            <mfrac>
                            <mrow>
                                <mo>(</mo><mi>k</mi><mn>1</mn><mo>+</mo><mn>1</mn><mo>)</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub>
                            </mrow>
                            <mrow>
                                <mi>k</mi><mn>1</mn>
                                <mo>(</mo>
                                <mn>1</mn><mo>&#x2212;</mo><mi>b</mi><mo>+</mo><mi>b</mi>
                                <mfrac>
                                    <mi>dl</mi><mi>avgdl</mi>
                                </mfrac>
                                <mo>)</mo>
                                <mo>+</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub>
                            </mrow>
                            </mfrac>
                            <mo>&#x22C5;</mo>
                            <!-- query‐side TF -->
                            <mfrac>
                            <mrow>
                                <mo>(</mo><mi>k</mi><mn>3</mn><mo>+</mo><mn>1</mn><mo>)</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub>
                            </mrow>
                            <mrow>
                                <mi>k</mi><mn>3</mn><mo>+</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub>
                            </mrow>
                            </mfrac>
                            <mo>)</mo>
                        </mrow>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><mi>d</mi></math></td>
                            <td>document</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>q</mi></math></td>
                            <td>query</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>t</mi><mi>i</mi></msub></math></td>
                            <td>term in the query</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>N</mi></math></td>
                            <td>total number of documents in the collection</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>n</mi><mi>i</mi></msub></math></td>
                            <td>number of documents containing term <var>i</var></td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub></math></td>
                            <td>frequency of term <var>i</var> in document <var>d</var></td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub></math></td>
                            <td>frequency of term <var>i</var> in query <var>q</var></td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>dl</mi></math></td>
                            <td>length of document <var>d</var> (in terms)</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>avgdl</mi></math></td>
                            <td>average document length in the collection</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>k</mi><mn>1</mn></math>, <math><mi>b</mi></math></td>
                            <td>tunable parameters (typically k₁ ∈ [1.2, 2.0], b = 0.75)</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>k</mi><mn>3</mn></math></td>
                            <td>tunable parameter for query term weighting</td>
                        </tr>
                        <tr>
                            <td class="symbol">first exp.</td>
                            <td>IDF component (inverse document frequency)</td>
                        </tr>
                        <tr>
                            <td class="symbol">second exp.</td>
                            <td>TF component with document length normalization</td>
                        </tr>
                        <tr>
                            <td class="symbol">third exp.</td>
                            <td>query term frequency component</td>
                        </tr>
                        </table>
                    </details>
                    <p>
                        Query tf component is usually ignored with an assumption that real-world queries are short, which means it has
                        negligible effect on the final ranking.
                    </p>
                    <p>
                        <math>
                            <mrow>
                                <mfrac>
                                <mrow>
                                    <mo>(</mo>
                                    <msub><mi>k</mi><mn>1</mn></msub>
                                    <mo>+</mo>
                                    <mn>1</mn>
                                    <mo>)</mo>
                                    <msub>
                                    <mi>f</mi>
                                    <mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow>
                                    </msub>
                                </mrow>
                                <mrow>
                                    <msub><mi>k</mi><mn>1</mn></msub>
                                    <mo>+</mo>
                                    <msub>
                                    <mi>f</mi>
                                    <mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow>
                                    </msub>
                                </mrow>
                                </mfrac>
                            </mrow>
                        </math>
                        is responsible for term frequency saturation, where <math><mi>k1</mi></math> controls the "knee" of the curve. When you fold
                        length normalization, you replace <math><mi>k1</mi></math> in that denominator with
                        <math>
                            <mrow>
                            <mi>K</mi>
                            <mo>=</mo>
                            <msub><mi>k</mi><mn>1</mn></msub>
                            <mo>&#x22C5;</mo>
                            <mrow>
                                <mo>(</mo>
                                <mn>1</mn><mo>&#x2212;</mo><mi>b</mi>
                                <mo>+</mo>
                                <mi>b</mi>
                                <mo>&#x22C5;</mo>
                                <mfrac>
                                <mi>dl</mi>
                                <mi>avgdl</mi>
                                </mfrac>
                                <mo>)</mo>
                            </mrow>
                            </mrow>
                        </math>, so long documents (large <math><mi>dl</mi></math>) get a bigger pseudo‑count <math><mi>K</mi></math>, which delays saturation,
                        with <math><mi>b</mi></math> as its knob.
                    </p>
                </li>
            </ul>

            <h2>Language-Model (LM) based</h2>
            <p>
                Language models are mathematical representations of text. Early (1950s) language modeling focused on rule-based grammars.
                In the 1980s, statistical approaches were explored and discrete representations like <i>n-grams</i>, with probabilities for discrete
                combinations of words, were found to be more useful than their rule-based predecessors.

                The simplest n-gram model, <i>unigram</i>, is a probability distribution over the words in the language. Higher-order n-gram models,
                including <i>bigram</i> and <i>trigram</i>, extend the same idea but quickly become data-hungry.
            </p>

            <table class="table">
                <thead>
                <tr>
                    <th>model</th>
                    <th>dependency span</th>
                    <th>parameter shape</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>unigram</td>
                    <td>none</td>
                    <td>vector</td>
                </tr>
                <tr>
                    <td>bigram</td>
                    <td>1 previous word</td>
                    <td>matrix</td>
                </tr>
                <tr>
                    <td>trigram</td>
                    <td>2 previous words</td>
                    <td>3-D tensor</td>
                </tr>
                </tbody>
            </table>

            <p>
                <i>unigram</i> models are simpler and proven to be effective, hence have been used widely as the basis for ranking algorithms.
            </p>
            <ul>
                <li>Query-likelihood with Dirichlet or Jelinek-Mercer smoothing (2000s)</li>
                <li>RM3 pseudo-relevance feedback</li>
            </ul>

            <h2>Learned-to-Rank (feature-based GBDT, LambdaMART)  (2007-2015)</h2>

            <h2>Neural IR (deep learning era)</h2>
            <ul>
                <li>Dual/Bi-Encoder dense retrieval (DPR, Contriever, etc.)</li>
                <li>Late-interaction (ColBERT, ColBERT-v2)</li>
                <li>Sparse-expansion (DeepCT, **SPLADE**)</li>
                <li>Cross-encoders (BERT re-rankers)</li>
                <li>Hybrid fusion (dense + BM25, dense + SPLADE)</li>
            </ul>

            <h2>Generative & Large-LM-centric</h2>
            <ul>
                <li>Retrieval-Augmented Generation (**RAG**) – encoder+generator loop</li>
                <li>Generative-retrieval (seq2seq decodes DocIDs; GRT, RIPOR, TIGER)</li>
                <li>HyDE/synthetic-query index enrichment</li>
            </ul>

            <h2>Summary</h2>
            <table class="table">
              <thead>
                <tr>
                  <th>Time Period</th>
                  <th>Development</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>1960s-70s</strong></td>
                  <td>Symbolic Boolean → users had to know query logic; recall-oriented.</td>
                </tr>
                <tr>
                  <td><strong>1975–1990s</strong></td>
                  <td>Vector-space & tf-idf made ranking automatic, enabling early commercial engines (e.g., SMART).</td>
                </tr>
                <tr>
                  <td><strong>1994–2005</strong></td>
                  <td>Probabilistic BM25 tuned term saturation & doc-length bias; became the web-search work-horse.</td>
                </tr>
                <tr>
                  <td><strong>2000-2010</strong></td>
                  <td>LM approaches tightened probabilistic grounding; fueled research but limited industry uptake (complexity, smoothing).</td>
                </tr>
                <tr>
                  <td><strong>2015-2020</strong></td>
                  <td>Word-embedding era; neural dual encoders hit production in QA (DPR 2020).</td>
                </tr>
                <tr>
                  <td><strong>2020-2023</strong></td>
                  <td>Late-interaction & sparse-expansion bridged efficiency/effectiveness gap; Lucene-ANN & ColBERT-v2 show commodity-hardware viability.</td>
                </tr>
                <tr>
                  <td><strong>2023-2024</strong></td>
                  <td>Vector-DB boom + corporate RAG adoption (Microsoft, Google, Cohere) mainstream dense retrieval in business apps.</td>
                </tr>
                <tr>
                  <td><strong>2024-2025+</strong></td>
                  <td>Generative retrieval prototypes demonstrate small-collection wins but face update & scaling hurdles; research racing to 100 M+-doc benchmarks.</td>
                </tr>
              </tbody>
            </table>

            <h2>References</h2>
            <div class="citation">
              <p id="ref1"><sup>1</sup> <a href="https://ciir.cs.umass.edu/irbook/" target="_blank">Search Engines Information Retrieval in Practice</a>, University of Massachusetts Amherst, 2010.</p>
            </div>
        </section>
        <!-- ────────────────────  FOOTER ICONS  ──────────────────── -->
        <footer class="icon-footer">
            <nav class="icon-nav">
            <a href="https://github.com/adamsoliev" target="_blank" title="GitHub" class="icon-link"><img src="../assets/logos/github.png" alt="GitHub" /></a>
            <a href="https://x.com/adamsoliev" target="_blank" title="X/Twitter" class="icon-link"><img src="../assets/logos/twitter.png" alt="X" /></a>
            <a href="mailto:adamsoliev.se@gmail.com" title="Email" class="icon-link"><img src="../assets/logos/email.png" alt="Email" /></a>
            </nav>
        </footer>
    </div>
    </body>
  </html>
