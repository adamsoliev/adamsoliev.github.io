<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Adam Soliev – Portfolio</title>
    <!-- Typeface -->
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../style.css" />
    <link rel="icon" type="image/svg+xml" href="../assets/favicon.svg">
  </head>
  <script>
    function setActiveFromHash() {
      const hash = location.hash || "#projects";
      document.querySelectorAll(".nav-link").forEach(l =>
        l.classList.toggle("active", l.getAttribute("href") === hash)
      );
    }
    window.addEventListener("hashchange", setActiveFromHash);
    setActiveFromHash(); // run on initial load
  </script>
  <body class="ibm-plex-mono-regular">
    <!-- ─────────────────────  SINGLE SHEET  ───────────────────── -->
    <div class="sheet">
        <!-- ────────────────────  TOP BAR  ──────────────────── -->
        <header class="top-bar">
          <div class="name-colors">
            <h1 class="site-name">Adam Soliev</h1>
            <div class="color-bar">
              <!-- Color rectangles to mimic the reference image -->
              <div class="color-rect red"></div>
              <div class="color-rect green"></div>
              <div class="color-rect blue"></div>
              <div class="color-rect cyan"></div>
              <div class="color-rect yellow"></div>
              <div class="color-rect magenta"></div>
              <div class="color-rect black"></div>
              <div class="color-rect light-gray"></div>
              <div class="color-rect gray"></div>
              <div class="color-rect darker-gray"></div>
              <div class="color-rect darkest-gray"></div>
              <div class="color-rect dark-gray"></div>
              <div class="color-rect black-2"></div>
              <div class="color-rect light-green"></div>
              <div class="color-rect dark-green"></div>
              <div class="color-rect navy"></div>
              <div class="color-rect royal-blue"></div>
              <div class="color-rect maroon"></div>
              <div class="color-rect brown"></div>
              <div class="color-rect orange"></div>
            </div>
          </div>
          <nav class="main-nav" aria-label="Primary">
            <a href="../index.html" class="nav-link">Projects</a>
            <a href="../index.html#blog" class="nav-link active">Blog</a>
            <a href="../index.html#about" class="nav-link">About</a>
          </nav>
        </header>

        <!-- ─────────────────────  POST  ───────────────────── -->
        <section class="post">
            <h1>Retrieval Models</h1>
            <p><i>1 May 2025</i></p>

            <p>What is a retrieval model? <i>A retrieval model</i> is a mathematical framework that describes how information is retrieved
                from a collection of documents, providing a way to measure the [topical, user-specific and binary or multi-valued] relevance of a document to a given query. <i>A ranking algorithm</i>
                then ranks the documents based on their relevance. </p>
            <h2>Symbolic/Exact-match</h2>
            <ul>
                <li>Boolean retrieval (1960s) - assumes that each retrieved document needs to contain all terms of the query. Otherwise,
                    the document is non-relevant. It doesn't differentiate between retrieved documents (they are all assumed to be relevant) and
                    has no notion of ranking. It also puts search-efficiency burden on the user since simple queries don't work well.</li>
            </ul>

            <h2>Algebraic weighting</h2>
            <ul>
                <li>
                    <p>
                        Vector space (1960s-1970s)
                        - simply put, this model assumes that documents and queries are points in a high-dimensional weight space.
                        In this context, a distance between document-and-query points can be taken as a measure of relevance. More commonly though some sort of
                        similarity measure is used. The most successful such measure is the <i>cosine correlation</i>,
                        which measures the cosine of the angle between the document and query vectors and is defined as:
                    </p>

                    <!-- Cosine(Dᵢ, Q)  -------------------------------------------------------->
                    <math display="block">
                        <mrow>
                        <!-- Cosine(D_i, Q) -->
                        <mi>Cosine</mi><mo>(</mo><msub><mi>D</mi><mi>i</mi></msub><mo>,</mo><mi>Q</mi><mo>)</mo>
                        <mo>=</mo>

                        <!--  ─────────────────────────────────────────────────────────────────── -->
                        <mfrac>

                            <!-- Numerator:  Σ_{j=1}^{t} d_{ij}·q_j -->
                            <mrow>
                            <munderover> <mo>&#x2211;</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>t</mi>
                            </munderover>
                            <msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub>
                            <mo>&#x2062;</mo>  <!-- invisible multiplication -->
                            <msub><mi>q</mi><mi>j</mi></msub>
                            </mrow>

                            <!-- Denominator:  √( Σ d_{ij}² · Σ q_j² ) -->
                            <mrow>
                            <msqrt>
                                <mrow>
                                <munderover> <mo>&#x2211;</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>t</mi>
                                </munderover>
                                <msubsup><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup>
                                <mo>&#x2062;</mo>  <!-- invisible multiplication -->
                                <munderover> <mo>&#x2211;</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>t</mi>
                                </munderover>
                                <msubsup><mi>q</mi><mi>j</mi><mn>2</mn></msubsup>
                                </mrow>
                            </msqrt>
                            </mrow>

                        </mfrac>
                        <!--  ─────────────────────────────────────────────────────────────────── -->
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><msub><mi>D</mi><mi>i</mi></msub></math></td>
                            <td>vector of term-weights for document <var>i</var></td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><mi>Q</mi></math></td>
                            <td>vector of term-weights for the query</td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></math></td>
                            <td>weight of term&nbsp;<var>j</var> in document&nbsp;<var>i</var></td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><msub><mi>q</mi><mi>j</mi></msub></math></td>
                            <td>weight of term&nbsp;<var>j</var> in the query</td>
                        </tr>

                        <tr>
                            <td class="symbol"><math><mi>t</mi></math></td>
                            <td>number of distinct terms (vector dimensionality)</td>
                        </tr>
                        <tr>
                            <td class="symbol">numerator</td>
                            <td>dot product of D and Q vectors</td>
                        </tr>
                        <tr>
                            <td class="symbol">denominator</td>
                            <td>product of Euclidean lengths of D and Q vectors</td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        Note from the formula that this model does take into account term weights and the number of matching terms, which
                        isn't possible in Boolean retrieval.
                    </p>
                    <p>
                        Most commonly used term weights, for both document and query vectors, are based on variations of <i>tf-idf</i>. The <i>tf</i>
                        reflects the importance of a term in a document or query, while the <i>idf</i> reflects the importance of a term in the collection
                        of documents (if it occurs in more documents, it is less differentiating).
                    </p>
            </ul>

            <h2>Probabilistic Models</h2>
            <ul>
                <li>
                    <p>
                        Binary-Independence (1970s-1980s) - treats a document as a bag of words, each encoded as either 1 (the query term is in the document) or 0, and
                        makes a strong assumption that terms are independent of each other. It defines document relevance as
                        <math>
                            <mi>P</mi><mo>(</mo><mi>R</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                            <mo>></mo>
                            <mi>P</mi><mo>(</mo><mi>NR</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                        </math>.
                        Applying Bayes' rule to each side and removing cancelled <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>)</mo></math>, you get:
                    </p>
                    <math display="block">
                        <mrow>
                            <mfrac>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>R</mi><mo>)</mo>
                                </mrow>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>NR</mi><mo>)</mo>
                                </mrow>
                            </mfrac>
                            <mo>></mo>
                            <mfrac>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>NR</mi><mo>)</mo>
                                </mrow>
                                <mrow>
                                    <mi>P</mi><mo>(</mo><mi>R</mi><mo>)</mo>
                                </mrow>
                            </mfrac>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol">leftside</td>
                            <td>likelihood ratio</td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        The likelihood ratio tells us how likely a document belongs to the relevant set and hence can be used to rank documents.
                        Keeping the aforementioned assumptions (binary and independence) in mind, you can estimate
                        <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>R</mi><mo>)</mo> </math>  by the product
                        of the individual term probabilities
                        <math>
                        <mrow>
                            <munderover>
                            <mo>&prod;</mo>
                            <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                            <mi>t</mi>
                            </munderover>
                            <mi>P</mi><mo>(</mo><msub><mi>d</mi><mi>i</mi></msub><mo>|</mo><mi>R</mi><mo>)</mo>
                        </mrow>
                        </math>
                        (and similarly for <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>NR</mi><mo>)</mo> </math>).
                    </p>
                    <p>
                        Another way to express the score is:
                    </p>

                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <mrow style="margin-right:10px;">
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <msub><mi>p</mi><mi>i</mi></msub>
                            <msub><mi>s</mi><mi>i</mi></msub>
                            </mfrac>
                        </mrow>
                        <!-- <mo>&#x2062;</mo> -->
                        <mo>&#x22C5;</mo>
                        <mrow>
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <mrow>
                                <mo>(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>p</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            <mrow>
                                <mo>(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            </mfrac>
                        </mrow>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><msub><mi>p</mi><mi>i</mi></msub></math></td>
                            <td>prob. term <var>i</var> occurs in a document from the rel. set</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>s</mi><mi>i</mi></msub></math></td>
                            <td>prob. term <var>i</var> occurs in a document from the non-rel. set</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>d</mi><mi>i</mi></msub></math></td>
                            <td>binary indicator (1 or 0)</td>
                        </tr>
                        <tr>
                            <td class="symbol">first product</td>
                            <td>contribution from terms present in the document</td>
                        </tr>
                        <tr>
                            <td class="symbol">second product</td>
                            <td>contribution from terms absent from the document</td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        With a bit of math manipulation, you can rewrite it as:
                    </p>
                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <!-- product over terms present -->
                        <mrow style="margin-right:10px;">
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <mrow>
                                <msub><mi>p</mi><mi>i</mi></msub>
                                <mo stretchy="false">(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo>
                            </mrow>
                            <mrow>
                                <msub><mi>s</mi><mi>i</mi></msub>
                                <mo stretchy="false">(</mo><mn>1</mn><mo>&#x2212;</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo>
                            </mrow>
                            </mfrac>
                        </mrow>
                        <mo>&#x22C5;</mo>
                        <!-- product over terms absent -->
                        <mrow>
                            <munder>
                            <mo>&#x220F;</mo>
                            <mrow>
                                <mi>i</mi><mo>:</mo>
                                <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn>
                            </mrow>
                            </munder>
                            <mfrac>
                            <mrow>
                                <mn>1</mn><mo>&#x2212;</mo><msub><mi>p</mi><mi>i</mi></msub>
                            </mrow>
                            <mrow>
                                <mn>1</mn><mo>&#x2212;</mo><msub><mi>s</mi><mi>i</mi></msub>
                            </mrow>
                            </mfrac>
                        </mrow>
                        </mrow>
                    </math>

                    <p>
                        The product on the right is identical for every document, so you drop it. To avoid accuracy problems caused by
                        working with small numbers, you use the logarithm (which converts the product to a sum):
                    </p>
                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <munder>
                            <mo>∑</mo>
                            <mrow>
                            <mi>i</mi><mo>:</mo>
                            <msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn>
                            </mrow>
                        </munder>
                        <mi>log</mi>
                        <mrow>
                            <mfrac>
                            <mrow>
                                <msub><mi>p</mi><mi>i</mi></msub>
                                <mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            <mrow>
                                <msub><mi>s</mi><mi>i</mi></msub>
                                <mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mo>)</mo>
                            </mrow>
                            </mfrac>
                        </mrow>
                        </mrow>
                    </math>

                    <p>If nothing is known about relevant documents, set
                        <math>
                        <msub><mi>p</mi><mi>i</mi></msub>
                        <mo>=</mo>
                        <mn>0.5</mn>
                        </math>
                        (maximum uncertainty) and approximate
                        <math>
                        <msub><mi>s</mi><mi>i</mi></msub>
                        <mo>≈</mo>
                        <mfrac>
                            <msub><mi>n</mi><mi>i</mi></msub>
                            <mi>N</mi>
                        </mfrac>
                        </math>
                        (collection frequency). Plugging those into the weight gives:
                    </p>

                    <math display="block">
                        <mrow>
                        <mo>=</mo>
                        <mi>log</mi>
                        <mrow>
                            <mo>(</mo>
                            <mfrac>
                            <mrow>
                                <mn>0.5</mn>
                                <mo stretchy="false">(</mo>
                                <mn>1</mn>
                                <mo>&#x2212;</mo>
                                <mfrac>
                                <msub><mi>n</mi><mi>i</mi></msub>
                                <mi>N</mi>
                                </mfrac>
                                <mo stretchy="false">)</mo>
                            </mrow>
                            <mrow>
                                <mfrac>
                                <msub><mi>n</mi><mi>i</mi></msub>
                                <mi>N</mi>
                                </mfrac>
                                <mo stretchy="false">(</mo>
                                <mn>1</mn>
                                <mo>&#x2212;</mo>
                                <mn>0.5</mn>
                                <mo stretchy="false">)</mo>
                            </mrow>
                            </mfrac>
                            <mo>)</mo>
                            <mo>=</mo>
                            <mi>log</mi>
                            <mrow>
                            <mo>(</mo>
                            <mfrac>
                                <mrow>
                                <mi>N</mi>
                                <mo>&#x2212;</mo>
                                <msub><mi>n</mi><mi>i</mi></msub>
                                </mrow>
                                <msub><mi>n</mi><mi>i</mi></msub>
                            </mfrac>
                            <mo>)</mo>
                            </mrow>
                        </mrow>
                        </mrow>
                    </math>

                    <p>
                        which is exactly the inverse‑document‑frequency (IDF) weight. Thus BIM naturally collapses to “sum of IDFs” when
                        no relevance feedback is available.
                    </p>

                </li>
                <li>
                    <p>
                        BM25 family (1990s) - a logical extension of BIM where term frequency and document length are taken into account, in addition to IDF.
                    </p>
                    <math display="block">
                        <mrow>
                        <mi>BM25</mi><mo>(</mo><mi>d</mi><mo>,</mo><mi>q</mi><mo>)</mo>
                        <mo>=</mo>
                        <munder>
                            <mo>&#x2211;</mo>
                            <mrow>
                            <msub><mi>t</mi><mi>i</mi></msub>
                            <mo>&#x2208;</mo>
                            <mi>q</mi>
                            </mrow>
                        </munder>
                        <mrow>
                            <mi>log</mi>
                            <mo>(</mo>
                            <!-- IDF term -->
                            <mfrac>
                            <mrow>
                                <mi>N</mi><mo>&#x2212;</mo><msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mn>0.5</mn>
                            </mrow>
                            <mrow>
                                <msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mn>0.5</mn>
                            </mrow>
                            </mfrac>
                            <mo>)</mo>
                            <mo>&#x22C5;</mo>
                            <!-- document‐side TF/length normalization -->
                            <mfrac>
                            <mrow>
                                <mo>(</mo><mi>k</mi><mn>1</mn><mo>+</mo><mn>1</mn><mo>)</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub>
                            </mrow>
                            <mrow>
                                <mi>k</mi><mn>1</mn>
                                <mo>(</mo>
                                <mn>1</mn><mo>&#x2212;</mo><mi>b</mi><mo>+</mo><mi>b</mi>
                                <mfrac>
                                    <mi>dl</mi><mi>avgdl</mi>
                                </mfrac>
                                <mo>)</mo>
                                <mo>+</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub>
                            </mrow>
                            </mfrac>
                            <mo>&#x22C5;</mo>
                            <!-- query‐side TF -->
                            <mfrac>
                            <mrow>
                                <mo>(</mo><mi>k</mi><mn>3</mn><mo>+</mo><mn>1</mn><mo>)</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub>
                            </mrow>
                            <mrow>
                                <mi>k</mi><mn>3</mn><mo>+</mo>
                                <msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub>
                            </mrow>
                            </mfrac>
                        </mrow>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><mi>d</mi></math></td>
                            <td>document</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>q</mi></math></td>
                            <td>query</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>t</mi><mi>i</mi></msub></math></td>
                            <td>term in the query</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>N</mi></math></td>
                            <td>total number of documents in the collection</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>n</mi><mi>i</mi></msub></math></td>
                            <td>number of documents containing term <var>i</var></td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub></math></td>
                            <td>frequency of term <var>i</var> in document <var>d</var></td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><msub><mi>f</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub></math></td>
                            <td>frequency of term <var>i</var> in query <var>q</var></td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>dl</mi></math></td>
                            <td>length of document <var>d</var> (in terms)</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>avgdl</mi></math></td>
                            <td>average document length in the collection</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>k</mi><mn>1</mn></math>, <math><mi>b</mi></math></td>
                            <td>tunable parameters (typically k₁ ∈ [1.2, 2.0], b = 0.75)</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>k</mi><mn>3</mn></math></td>
                            <td>tunable parameter for query term weighting</td>
                        </tr>
                        <tr>
                            <td class="symbol">first exp.</td>
                            <td>IDF component (inverse document frequency)</td>
                        </tr>
                        <tr>
                            <td class="symbol">second exp.</td>
                            <td>TF component with document length normalization</td>
                        </tr>
                        <tr>
                            <td class="symbol">third exp.</td>
                            <td>query term frequency component</td>
                        </tr>
                        </table>
                    </details>
                    <p>
                        Query tf component is usually ignored with an assumption that real-world queries are short, which means it has
                        negligible effect on the final ranking.
                    </p>
                    <p>
                        <math>
                            <mrow>
                                <mfrac>
                                <mrow>
                                    <mo>(</mo>
                                    <msub><mi>k</mi><mn>1</mn></msub>
                                    <mo>+</mo>
                                    <mn>1</mn>
                                    <mo>)</mo>
                                    <msub>
                                    <mi>f</mi>
                                    <mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow>
                                    </msub>
                                </mrow>
                                <mrow>
                                    <msub><mi>k</mi><mn>1</mn></msub>
                                    <mo>+</mo>
                                    <msub>
                                    <mi>f</mi>
                                    <mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow>
                                    </msub>
                                </mrow>
                                </mfrac>
                            </mrow>
                        </math>
                        is responsible for term frequency saturation, where <math><mi>k1</mi></math> controls the "knee" of the curve. When you fold
                        length normalization, you replace <math><mi>k1</mi></math> in that denominator with
                        <math>
                            <mrow>
                            <mi>K</mi>
                            <mo>=</mo>
                            <msub><mi>k</mi><mn>1</mn></msub>
                            <mo>&#x22C5;</mo>
                            <mrow>
                                <mo>(</mo>
                                <mn>1</mn><mo>&#x2212;</mo><mi>b</mi>
                                <mo>+</mo>
                                <mi>b</mi>
                                <mo>&#x22C5;</mo>
                                <mfrac>
                                <mi>dl</mi>
                                <mi>avgdl</mi>
                                </mfrac>
                                <mo>)</mo>
                            </mrow>
                            </mrow>
                        </math>, so long documents (large <math><mi>dl</mi></math>) get a bigger pseudo‑count <math><mi>K</mi></math>, which delays saturation,
                        with <math><mi>b</mi></math> as its knob.
                    </p>
                </li>
            </ul>

            <h2>Language-Model (LM) based</h2>
            <p>
                Language models are mathematical representations of text. Early (1950s) language modeling focused on rule-based grammars.
                In the 1980s, statistical approaches were explored and discrete representations like <i>n-grams</i>, with probabilities for discrete
                combinations of words, were found to be more useful than their rule-based predecessors.

                The simplest n-gram model, <i>unigram</i>, is a probability distribution over the words in the language. Higher-order n-gram models,
                including <i>bigram</i> and <i>trigram</i>, extend the same idea but quickly become data-hungry.
            </p>

            <table class="table">
                <thead>
                <tr>
                    <th>model</th>
                    <th>dependency span</th>
                    <th>parameter shape</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>unigram</td>
                    <td>none</td>
                    <td>vector</td>
                </tr>
                <tr>
                    <td>bigram</td>
                    <td>1 previous word</td>
                    <td>matrix</td>
                </tr>
                <tr>
                    <td>trigram</td>
                    <td>2 previous words</td>
                    <td>3-D tensor</td>
                </tr>
                </tbody>
            </table>

            <p>
                <i>unigram</i> models are simpler and proven to be effective, hence have been used widely as the basis for ranking algorithms.
                Note throughout this discussion that LMs leverage topical relevance of the text, which is only a part of the answer to affective search.
            </p>
            <ul>
                <li>
                    <p>
                        Query-likelihood with Jelinek-Mercer or Dirichlet smoothing (2000s) - in this model, the core intuition is to rank
                        documents based on high likely the query could be generated by the document LM. Essentially, given a query, you want to calculate
                        <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>Q</mi><mo>)</mo></math> for each document. It isn't possible
                        directly, but Bayes' Rule lets you turn it into something computable:
                    </p>
                    <math display="block">
                        <mrow>
                        <mi>P</mi><mo>(</mo><mi>D</mi><mo>|</mo><mi>Q</mi><mo>)</mo>
                        <mo>=</mo>
                        <mfrac>
                            <mrow>
                            <mi>P</mi><mo>(</mo><mi>Q</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                            <mo>&#x22C5;</mo>
                            <mi>P</mi><mo>(</mo><mi>D</mi><mo>)</mo>
                            </mrow>
                            <mrow>
                            <mi>P</mi><mo>(</mo><mi>Q</mi><mo>)</mo>
                            </mrow>
                        </mfrac>
                        </mrow>
                    </math>

                    <p>
                        <math><mi>P</mi><mo>(</mo><mi>Q</mi><mo>)</mo></math>
                        is the same for all <math><mi>D</mi></math> and
                        <math><mi>P</mi><mo>(</mo><mi>D</mi><mo>)</mo></math>
                        is assumed to be uniform (could be non-uniform based on document date, length, etc.),
                        so both are ignored. With these assumptions, the ranking is now determined by
                        <math><mi>P</mi><mo>(</mo><mi>Q</mi><mo>|</mo><mi>D</mi><mo>)</mo></math>,
                        which we can compute using the unigram LM for the document:
                    </p>

                    <math display="block">
                        <mrow>
                            <mi>P</mi><mo>(</mo><mi>Q</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                        <mo>=</mo>
                        <munderover>
                            <mo>&prod;</mo>
                            <mrow><mi>i</mi><mo>=</mo> <mn>1</mn></mrow>
                            <mi>n</mi>
                        </munderover>
                        <mi>P</mi><mo>(</mo>
                        <msub><mi>q</mi> <mi>i</mi></msub>
                        <mo>|</mo><mi>D</mi><mo>)</mo>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><msub><mi>q</mi><mi>i</mi></msub></math></td>
                            <td>query word</td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mi>n</mi></math></td>
                            <td>words in the query</td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        For individual probabilities, you can use the word frequency:
                    </p>
                    <math display="block">
                        <mrow>
                        <mi>P</mi><mo>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo><mi>D</mi><mo>)</mo>
                        <mo>=</mo>
                        <mfrac>
                            <msub>
                            <mi>f</mi>
                            <mrow><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><mi>D</mi></mrow>
                            </msub>
                            <mrow>
                            <mo>|</mo><mi>D</mi><mo>|</mo>
                            </mrow>
                        </mfrac>
                        </mrow>
                    </math>

                    <!-- Explanations -->
                    <details class="where-details">
                        <summary>where:</summary>
                        <table class="where-table">
                        <tr>
                            <td class="symbol"><math><msub><mi>f</mi> <mrow><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><mi>D</mi></mrow></msub></math></td>
                            <td>number of times the term <math><msub><mi>q</mi><mi>i</mi></msub></math> appears in <math><mi>D</mi></math></td>
                        </tr>
                        <tr>
                            <td class="symbol"><math><mo>|</mo><mi>D</mi><mo>|</mo></math></td>
                            <td>number of words in <math><mi>D</mi></math></td>
                        </tr>
                        </table>
                    </details>

                    <p>
                        There are a number of issues with this estimate. First, if any of the query words are missing from the document,
                        the score for the whole document will be zero. Second, there is no distinction between documents that have different numbers of
                        query words missing. Third, words that aren't in the document but are related to the query should have non-zero probability
                        in the document LM.
                    </p>
                    <p>
                        <i>Smoothing</i> addresses these issues. One common technique is to lower probability estimates for words in the document by
                        <math><msub><mi>α</mi><mi>D</mi></msub></math> and assign those
                        "leftover" probability estimates to words in the collection, which has seen and unseen words in <math><mi>D</mi></math>,
                        proportionally to the collection probabilities. This results in:
                    </p>
                    <math display="block">
                      <mrow>
                        <msub><mi>P</mi><mi>seen</mi></msub>
                        <mo>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo><mi>D</mi><mo>)</mo>
                        <mo>=</mo>
                        <mrow>
                          <mo>(</mo>
                            <mn>1</mn><mo>−</mo>
                            <msub><mi>α</mi><mi>D</mi></msub>
                          <mo>)</mo>
                          <mo>&#x22C5;</mo>
                          <mi>P</mi>
                          <mo>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo><mi>D</mi><mo>)</mo>
                          <mo>+</mo>
                          <msub><mi>α</mi><mi>D</mi></msub>
                          <mo>&#x22C5;</mo>
                          <mi>P</mi>
                          <mo>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo><mi>C</mi><mo>)</mo>
                        </mrow>
                      </mrow>
                    </math>
                    <math display="block">
                      <mrow>
                        <msub><mi>P</mi><mi>unseen</mi></msub>
                        <mo>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo><mi>D</mi><mo>)</mo>
                        <mo>=</mo>
                        <mrow>
                          <msub><mi>α</mi><mi>D</mi></msub>
                          <mo>&#x22C5;</mo>
                          <mi>P</mi>
                          <mo>(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>|</mo><mi>C</mi><mo>)</mo>
                        </mrow>
                      </mrow>
                    </math>
                    <p>
                        Depending on how you specify the value of <math><msub><mi>&alpha;</mi><mi>D</mi></msub></math>, different forms of estimation emerge.
                        <i>Jelinek-Mercer</i> method assumes it to be constant (e.g., <math><msub><mi>α</mi><mi>D</mi></msub><mo>=</mo><mi>λ</mi></math>), resulting in the following query likelihood model:
                    </p>
                    <math display="block">
                      <mrow>
                        <mi>P</mi><mo>(</mo><mi>Q</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                        <mo>=</mo>
                        <munderover>
                          <mo>∏</mo>
                          <mrow>
                            <mi>i</mi><mo>=</mo><mn>1</mn>
                          </mrow>
                          <mi>n</mi>
                        </munderover>
                        <mo>(</mo>
                          <mrow>
                            <mo>(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo>)</mo>
                            <mfrac>
                              <mrow>
                                <msub>
                                  <mi>f</mi>
                                  <mrow>
                                    <msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><mi>D</mi>
                                  </mrow>
                                </msub>
                              </mrow>
                              <mrow>
                                <mo>|</mo><mi>D</mi><mo>|</mo>
                              </mrow>
                            </mfrac>
                            <mo>+</mo>
                            <mi>λ</mi>
                            <mfrac>
                              <mrow>
                                <msub>
                                  <mi>c</mi>
                                  <msub><mi>q</mi><mi>i</mi></msub>
                                </msub>
                              </mrow>
                              <mrow>
                                <mo>|</mo><mi>C</mi><mo>|</mo>
                              </mrow>
                            </mfrac>
                          </mrow>
                        <mo>)</mo>
                      </mrow>
                    </math>
                    <p>
                        In its log form, it turns into:
                    </p>
                    <math display="block">
                      <mrow>
                        <mi>log</mi>
                        <mi>P</mi>
                        <mo>(</mo><mi>Q</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                        <mo>=</mo>
                        <munderover>
                          <mo>&#x2211;</mo>
                          <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                          <mi>n</mi>
                        </munderover>
                        <mrow>
                          <mi>log</mi>
                          <mo>(</mo>
                          <mrow>
                            <mo>(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo>)</mo>
                            <mfrac>
                              <mrow>
                                <msub><mi>f</mi><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><mi>D</mi></mrow></msub>
                              </mrow>
                              <mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow>
                            </mfrac>
                            <mo>+</mo>
                            <mi>λ</mi>
                            <mfrac>
                              <mrow>
                                <msub><mi>c</mi><mrow><msub><mi>q</mi><mi>i</mi></msub></mrow></msub>
                              </mrow>
                              <mrow><mo>|</mo><mi>C</mi><mo>|</mo></mrow>
                            </mfrac>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </math>

                    <p>
                        <i>Dirichlet</i> associates <math><msub><mi>α</mi><mi>D</mi></msub></math> to document length, which turns out to be a more effective,
                        using (<math><mi>μ</mi></math> is set empirically):
                    </p>
                    <math display="block">
                      <mrow>
                        <msub><mi>α</mi><mi>D</mi></msub>
                        <mo>=</mo>
                        <mfrac>
                          <mi>μ</mi>
                          <mrow>
                            <mo>|</mo><mi>D</mi><mo>|</mo>
                            <mo>+</mo>
                            <mi>μ</mi>
                          </mrow>
                        </mfrac>
                      </mrow>
                    </math>
                    <p>The resulting query likelihood model:</p>

                    <math display="block">
                      <mrow>
                        <mi>log</mi>
                        <mi>P</mi>
                        <mo>(</mo><mi>Q</mi><mo>|</mo><mi>D</mi><mo>)</mo>
                        <mo>=</mo>
                        <munderover>
                          <mo>&#x2211;</mo>
                          <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                          <mi>n</mi>
                        </munderover>
                        <mrow>
                          <mi>log</mi>
                          <mo>(</mo>
                          <mfrac>
                            <mrow>
                              <msub>
                                <mi>f</mi>
                                <mrow>
                                  <msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><mi>D</mi>
                                </mrow>
                              </msub>
                              <mo>+</mo>
                              <mi>μ</mi>
                              <mfrac>
                                <mrow>
                                  <msub>
                                    <mi>c</mi>
                                    <msub><mi>q</mi><mi>i</mi></msub>
                                  </msub>
                                </mrow>
                                <mrow><mo>|</mo><mi>C</mi><mo>|</mo></mrow>
                              </mfrac>
                            </mrow>
                            <mrow>
                              <mo>|</mo><mi>D</mi><mo>|</mo><mo>+</mo><mi>μ</mi>
                            </mrow>
                          </mfrac>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </math>
                </li>
                <p>
                    You can turn the query likelihood model upside down and come up with a document likelihood model. In that case, not only is it
                    more difficult to compute the scores because of greatly varying document lengths, but also there is an issue of obtaining relevant documents.
                    Instead of these, you can generalize the problem and directly compare the query LM with the document LM, using a measure like
                    <i>Kullback-Leibler divergence</i>, which measures the difference between two probability distributions.
                </p>
            </ul>

            <h2>Learning to Rank (2007-2015)</h2>
            <ul>
                <li>
                    <p>
                        Ranking SVM - one of the early successes in applying ML to information retrieval. Unlike previous methods,
                        such as logistic regression, which required extensive and costly data with explicit relevance judgments,
                        Ranking SVM leveraged easily accessible clickthrough data and focused on developing
                        an algorithm to learn a ranking function from this data.
                    </p>
                    <p>
                        Clickthrough data can be thought of as triplets
                        <math><mo>(</mo><mi>q</mi><mi>,</mi><mi>r</mi><mi>,</mi><mi>c</mi><mo>)</mo></math>
                        consisting of the query <math><mi>q</mi></math>, the ranking of
                        documents <math><mi>r</mi></math> presented to the user, and the set <math><mi>c</mi></math> of links the user clicked on. From these clicks,
                        you can infer the relative relevance of documents, e.g., if a user is shown 3 links and they clicked on link 3,
                        it is safe to assume that they preferred link 3 to link 2 and link 1, which means
                        <math><msub><mi>d</mi><mi>3</mi></msub><mo>></mo><msub><mi>d</mi><mi>2</mi></msub></math>
                        and
                        <math><msub><mi>d</mi><mi>3</mi></msub><mo>></mo><msub><mi>d</mi><mi>1</mi></msub></math>, respectively.
                    </p>
                    <p>
                        Given the above input and the fact that each document is represented as a feature vector
                        <math><mi>Φ</mi> <mo>(</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>j</mi></msub><mo>)</mo></math>,
                        Ranking SVM tries to find a weight vector <math><mover> <mi>w</mi> <mo>→</mo> </mover></math>
                        so that for as many pairwise preferences
                        <math><msub><mi>d</mi><mi>i</mi></msub><mo>></mo><msub><mi>d</mi><mi>j</mi></msub></math>
                        as possible,
                        <math>
                          <mrow>
                            <mover>
                              <mi>w</mi>
                              <mo>→</mo>
                            </mover>
                            <mo>·</mo>
                            <mi>Φ</mi>
                            <mo>(</mo><mi>q</mi><mo>,</mo>
                              <msub><mi>d</mi><mi>i</mi></msub>
                            <mo>)</mo>
                            <mo>&gt;</mo>
                            <mover>
                              <mi>w</mi>
                              <mo>→</mo>
                            </mover>
                            <mo>·</mo>
                            <mi>Φ</mi>
                            <mo>(</mo><mi>q</mi><mo>,</mo>
                              <msub><mi>d</mi><mi>j</mi></msub>
                            <mo>)</mo>
                          </mrow>
                        </math>.
                        Or equally,
                        <math>
                          <mrow>
                            <mover>
                              <mi>w</mi>
                              <mo>→</mo>
                            </mover>
                            <mo>·</mo>
                            <mo>(</mo>
                              <mi>Φ</mi>
                              <mo>(</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>i</mi></msub><mo>)</mo>
                              <mo>−</mo>
                              <mi>Φ</mi>
                              <mo>(</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>j</mi></msub><mo>)</mo>
                            <mo>)</mo>
                            <mo>&gt;</mo>
                            <mn>0</mn>
                          </mrow>
                        </math>.
                        This leads to, in SVM terms, hyperplane being assumed at
                        <math>
                            <mrow>
                                <mover>
                                <mi>w</mi>
                                <mo>→</mo>
                                </mover>
                                <mo>·</mo>
                                <mover>
                                <mi>x</mi>
                                <mo>→</mo>
                                </mover>
                                <mo>=</mo>
                                <mn>0</mn>
                            </mrow>
                        </math>
                        and each preference
                        (<math><msub><mi>d</mi><mi>i</mi></msub><mo>></mo><msub><mi>d</mi><mi>j</mi></msub></math>)
                        having margin of at least +1 (indicating <math><msub><mi>d</mi><mi>i</mi></msub></math> should be
                        ranked higher than <math><msub><mi>d</mi><mi>j</mi></msub></math>) away from the hyperplane.
                        Another way of saying the above goal is to maximize this margin, so that we can match the clickthrough
                        ranking as closely as possible.
                    </p>
                    <p>
                        From the distance formula for two parallel hyperplanes
                        <math display="block">
                          <mrow>
                            <mo>=</mo>
                            <mfrac>
                              <mrow>
                                <mo>|</mo>
                                <msub><mi>c</mi><mn>2</mn></msub>
                                <mo>−</mo>
                                <msub><mi>c</mi><mn>1</mn></msub>
                                <mo>|</mo>
                              </mrow>
                              <mrow>
                                <mo>∥</mo>
                                <mover><mi>w</mi><mo>→</mo></mover>
                                <mo>∥</mo>
                              </mrow>
                            </mfrac>
                            <mo>=</mo>
                            <mfrac>
                              <mrow>
                                <mn>1</mn>
                                <mo>−</mo>
                                <mn>0</mn>
                              </mrow>
                              <mrow>
                                <mo>∥</mo>
                                <mover><mi>w</mi><mo>→</mo></mover>
                                <mo>∥</mo>
                              </mrow>
                            </mfrac>
                          </mrow>
                        </math>
                        <details class="where-details">
                            <summary>where:</summary>
                            <table class="where-table">
                            <tr>
                                <td class="symbol"><math><msub><mi>c</mi><mn>i</mn></msub></math></td>
                                <td>bias terms, or e.g.,
                                    <math>
                                        <mrow>
                                            <mover>
                                            <mi>w</mi>
                                            <mo>→</mo>
                                            </mover>
                                            <mo>·</mo>
                                            <mover>
                                            <mi>x</mi>
                                            <mo>→</mo>
                                            </mover>
                                            <mo>=</mo>
                                            <msub><mi>c</mi><mn>1</mn></msub>
                                        </mrow>
                                    </math>
                                </td>
                            </tr>
                            </table>
                        </details>
                        you can see that maximizing margin is the same as minimizing <math><mover> <mi>w</mi> <mo>→</mo></mover></math>.
                        In other words, you can reformulate this problem as a standard SVM optimization as follows:
                        <math display="block">
                          <mrow>
                            <mtext>minimize:</mtext>
                            <mspace width="0.5em"/>
                            <mfrac>
                              <mn>1</mn>
                              <mn>2</mn>
                            </mfrac>
                            <mspace width="0.5em"/>
                            <mover>
                              <mi>w</mi>
                              <mo>→</mo>
                            </mover>
                            <mo>·</mo>
                            <mover>
                              <mi>w</mi>
                              <mo>→</mo>
                            </mover>
                            <mo>+</mo>
                            <mi>C</mi>
                            <munder>
                              <mo>∑</mo>
                              <mrow>
                                <mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi>
                              </mrow>
                            </munder>
                            <msub>
                              <mi>ξ</mi>
                              <mrow>
                                <mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi>
                              </mrow>
                            </msub>
                          </mrow>
                        </math>
                        <details class="where-details">
                            <summary>where:</summary>
                            <table class="where-table">
                            <tr>
                                <td class="symbol"><math><mi>ξ</mi></math></td>
                                <td><i>slack variable</i>, accomodates errors in training examples</td>
                            </tr>
                            <tr>
                                <td class="symbol"><math><mi>C</mi></math></td>
                                <td>parameter that is used to prevent overfitting</td>
                            </tr>
                            </table>
                        </details>
                    </p>
                    <p>
                        Note that a hyperplane is a decision boundary that separates data points into different sets; it's a line in
                        2D, a plane in 3D, and so on for higher dimensions.
                    </p>
                </li>
                <li>RankNet</li>
                <li>LambdaRank and LambdaMART</li>
            </ul>

            <h2>Neural IR (deep learning era)</h2>
            <ul>
                <li>Dual/Bi-Encoder dense retrieval (DPR, Contriever, etc.)</li>
                <li>Late-interaction (ColBERT, ColBERT-v2)</li>
                <li>Sparse-expansion (DeepCT, **SPLADE**)</li>
                <li>Cross-encoders (BERT re-rankers)</li>
                <li>Hybrid fusion (dense + BM25, dense + SPLADE)</li>
            </ul>

            <h2>Generative & Large-LM-centric</h2>
            <ul>
                <li>Retrieval-Augmented Generation (**RAG**) – encoder+generator loop</li>
                <li>Generative-retrieval (seq2seq decodes DocIDs; GRT, RIPOR, TIGER)</li>
                <li>HyDE/synthetic-query index enrichment</li>
            </ul>

            <h2>Summary</h2>
            <table class="table">
              <thead>
                <tr>
                  <th>Time Period</th>
                  <th>Development</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>1960s-70s</strong></td>
                  <td>Symbolic Boolean → users had to know query logic; recall-oriented.</td>
                </tr>
                <tr>
                  <td><strong>1975–1990s</strong></td>
                  <td>Vector-space & tf-idf made ranking automatic, enabling early commercial engines (e.g., SMART).</td>
                </tr>
                <tr>
                  <td><strong>1994–2005</strong></td>
                  <td>Probabilistic BM25 tuned term saturation & doc-length bias; became the web-search work-horse.</td>
                </tr>
                <tr>
                  <td><strong>2000-2010</strong></td>
                  <td>LM approaches tightened probabilistic grounding; fueled research but limited industry uptake (complexity, smoothing).</td>
                </tr>
                <tr>
                  <td><strong>2015-2020</strong></td>
                  <td>Word-embedding era; neural dual encoders hit production in QA (DPR 2020).</td>
                </tr>
                <tr>
                  <td><strong>2020-2023</strong></td>
                  <td>Late-interaction & sparse-expansion bridged efficiency/effectiveness gap; Lucene-ANN & ColBERT-v2 show commodity-hardware viability.</td>
                </tr>
                <tr>
                  <td><strong>2023-2024</strong></td>
                  <td>Vector-DB boom + corporate RAG adoption (Microsoft, Google, Cohere) mainstream dense retrieval in business apps.</td>
                </tr>
                <tr>
                  <td><strong>2024-2025+</strong></td>
                  <td>Generative retrieval prototypes demonstrate small-collection wins but face update & scaling hurdles; research racing to 100 M+-doc benchmarks.</td>
                </tr>
              </tbody>
            </table>

            <h2>References</h2>
            <div class="citation">
              <p id="ref1"><sup>1</sup> <a href="https://ciir.cs.umass.edu/irbook/" target="_blank">Search Engines Information Retrieval in Practice</a>, University of Massachusetts Amherst, 2010.</p>
            </div>
        </section>
        <!-- ────────────────────  FOOTER ICONS  ──────────────────── -->
        <footer class="icon-footer">
            <nav class="icon-nav">
            <a href="https://github.com/adamsoliev" target="_blank" title="GitHub" class="icon-link"><img src="../assets/logos/github.png" alt="GitHub" /></a>
            <a href="https://x.com/adamsoliev" target="_blank" title="X/Twitter" class="icon-link"><img src="../assets/logos/twitter.png" alt="X" /></a>
            <a href="mailto:adamsoliev.se@gmail.com" title="Email" class="icon-link"><img src="../assets/logos/email.png" alt="Email" /></a>
            </nav>
        </footer>
    </div>
    </body>
  </html>
