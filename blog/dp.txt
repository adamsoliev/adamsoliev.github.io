

Why does DP feel unintuitive?

Problem #1 - Coin
------------------------------------------
Suppose that we are given a set of coin values coins = [1,2,5] and a
target sum of money 11, and we are asked to construct the sum 11 using as few coins as
possible

What solution did come to your mind when you read the problem?

One example is a greedy algorithm where we always select the largest possible
coin. Looks reasonable but it does produce incorrect solution for coins =
{1,3,4} and target sum of 6 (4 + 1 + 1 vs 3 + 3)

DP solves this problem in an almost like brute force way - going through all possible ways
to select coins but does so in an efficient manner (memoization). More official
description of DP is it solves subproblems and builds up the final solution. 

In the above example, we can think of many ways to construct the sum of 11:
1 + 1 + ... + 1 = 11
1 + 1 + ... + 2 = 11
1 + 2 + ... + 2 = 11
1 + 1 + ... + 5 = 11
1 + 5 + ... + 5 = 11
...

This is easy to think but doesn't lead us anythere except brute force solution

Another way to think about this is through subproblems. Can we break down the
problem into smaller pieces and construct our final answer from them? If so,
smaller problem are easier to tackle and find optional solution to. For example,
if the target sum above was 0 or 1, we don't really have to think to find the
optional solution - 0 and 1. What about 2? it is also pretty easy for our
mind - 1 (since we have a coin with value of 2). what about 3? Notice that now
your mind doesn't know the answer right away. You need to think for a short bit
to find that the answer is 2 (1 + 2). Challenge and, I'd argue the beauty, of DP
is exactly this - easy to tackle subproblems. 

Solution to the above problem looks something like this
```python
def coinChange(self, coins: List[int], amount: int) -> int:
    dp = [float('inf')] * (amount + 1)
    dp[0] = 0

    for i in range(1, amount + 1):
        for coin in coins:
            if i - coin < 0: continue
            dp[i] = min(dp[i - coin] + 1, dp[i])
    
    if dp[amount] == float('inf'): return -1
    return dp[amount]
```
Notice that we start from amount 1 until 12 (Python's range end-exclusive). For
each subamount, we are finding the optimal solution by trying out all coins.
Notice that for each amount, we are going through coins list once and that gives us
the optional solution for that subamount (as opposed to in a brute force way, we'd generate all
possible combonations and pick ones with the target sum). In other words,
knowing optional solution to a smaller subproblem gives us an ability to find
an optimal solution to a larger subproblem far more cheaply, computationally,
than finding an optional solution in a brute force way. 

Sometimes, dp problem asks us to find min/max (min # of coins to make 11), an
example solution (an example of actual coin values that make up 11) or count
(how many coin combonations can make 11). To approach any of this, think of a
recursive function that let's you construct an optional solution to a problem
using solutions to subproblems. 



Problem #2 - Longest increasing subsequence
------------------------------------------
Subproblem here is a size of the array. Say array is [6,2,5,1,7,4,8,3], which
has a length of 8. Base subproblem here is length of 0, which has a solution of
0. length of 1 means 1. Length 2 means 

[]      => 0
[6]     => 1
[6,2]   => 1
[6,2,5] => 2
...
[6,2,5,1,7,4,8,3]

What is the recurring relationship between length and length - 1? It is whether
the latest added element is larger than one of its left neighbors or not. So
for any element, we should look to all its left neighbors and evaluate the
solution to the current subproblem. If we find that current element is larger
than ith element, that means that we can extend ith increasing subarray by 1 by
adding the current element.

```python
def lengthOfLIS(self, nums: List[int]) -> int:
  if not nums: return 0

  n = len(nums)
  dp = [1] * n    # max length is at least 1

  for k in range(1, n):
      for i in range(k):
          if nums[i] < nums[k]:
              dp[k] = max(dp[k], dp[i] + 1)
  return max(dp)
```

2D version of this is finding a largest sum path in a grid with a constraint
that we can only move right and down. What do you think the subproblem is in
this case? Can you write down the formula?

Answer is: sum(y, x)= max(sum(y, x - 1), sum(y - 1, x)) + grid[y][x], where sum
is a dp array. Can you make sense of it? It is based on the observation that a
path that ends at square (y, x) can come either from square (y, x - 1) or from
square (y - 1, x).


Problem #3 - Knapsack problems
------------------------------------------


You have a saving amount of $250 to invest in stocks, each with a specific cost
and potential sale value. Given the lists costs = [175, 133, 109, 177, 210] and
sales = [204, 128, 160, 211, 231], determine the maximum profit you can achieve
by buying and selling stocks without exceeding the saving limit. Each stock can
only be purchased once.

```python
def max_profit(saving, costs, sales):
    profits = [sales[i] - costs[i] for i in range(len(costs))]
    dp = [0] * (saving + 1)
    
    for budget in range(saving + 1):
        for i in range(len(costs)):
            if budget < costs[i]: continue
            dp[budget] = max(dp[budget], dp[budget - costs[i]] + profits[i])
    
    return dp[saving]
```

